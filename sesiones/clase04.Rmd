#Clase 04

#Conglomerados (Clusters)

No te va a dar un índice, pero te va a dar **agrupamientos**.

Mi producto final es cómo se organizan las provincias en grupos.

Lo importante de la técnica es: usando información de este tipo, **va a decidir qué grupos hay**.

[Si quiero ser Presidente: ganarme el que tenga un buen IDE. En cambio, si es que hay un mal IDE: ahí deben odiar al gobierno].

ÍNDICE DE DENSIDAD DEL ESTADO

- Identidad

- Salud

- Educación

- Saneamiento

- Electrificación

[Cuando están en un curso comigo, deben pensar en cualquier crueldad que se me pueda ocurrir.]

[Tienen que adelantarse a lo que me pueda ocurrir.]

[Si yo fuera él, preguntaría esto.]

***

Métodos de clusterización

- JERÁRQUICO: **No te dice cuántos clusters hay**

- K-MEDIAS: **Tú decides cuántos clusters quieres**

- ESPACIAL: 


Si tuviera una manera objetiva de organizar este grupo de alumnos. Pensemos en distancia geográfica.

Ancón versus La Molina... 

Jesús María - Lince

**DENDOGRAMA**

***

Primero analizar lo de IDE-PERÚ

[Revisar el Reporte del Índice de Densidad del Estado]

[Para tesis con clusters. **Cómo justifico que entre una variable en mi cluster.**]

***

**MËTODO JERÁRQUICO**

Link de la clase: https://rawgit.com/PoliticayGobiernoPUCP/EstadisticaPoliticaGobiernoII/master/sesiones/ClusterAnalysis_class_Peru.html

```{r}
library(openxlsx)
folder='data'
fileName='idePeru.xlsx'
fileToRead=file.path(folder,fileName)
cualHoja=1
datos<- read.xlsx(fileToRead, cualHoja)
head(datos,10)
```

**Openxlsx** es muy eficiente, pues inclusive omite columnas vacías. Sin embargo, vemos que la data no inicia donde debe, los titulos no corresponden a las columnas; y, aunque las Provincias figuran en cada fila, los Departamentos no están en las filas, sino como titulos para las provincias (y encima no hay distritos, aunque la hoja muestra ese nombre). Arreglemos esto.

```{r}
filaInicial=4
datos=read.xlsx(fileToRead, 
                sheet = 1, 
                startRow = filaInicial, 
                skipEmptyRows = TRUE, skipEmptyCols = TRUE)
```

Los reportes de este tipo sueles añadir información innecesaria al final, verifiquemos:

```{r}
tail(datos,10)
```

La data acaba en la fila 222, con la provincia de Purus. Eliminemos filas innecesarias:

```{r}
datos=datos[-c(223:226),]
```

El encabezado aun tiene problemas:

```{r}
head(datos)
```

Las filas 1 al 3 nos innecesarias:

```{r}
datos=datos[-c(1:3),]
```

Separemos y guardemos los nombres de regiones y sus codigos (ubigeos) :

```{r}
queColumnas=c(1,2)
regiones=datos[,queColumnas]
head(regiones,10)
```

Podemos quedarnos con los casos completos por fila:

```{r}
regiones=regiones[complete.cases(regiones),]
head(regiones,10)
```

Cambiemos nombres:

```{r}
nombresNuevos=c('regionUbigeo','regionNombre')
names(regiones)=nombresNuevos
head(regiones)
```

De igual manera, guardemos la info de las provincias:

```{r}
queColumnas=c(2)
provincias=datos[,-queColumnas]
head(provincias,10)
```

Veamos que columnas tenemos:

```{r}
names(provincias)
```

X1 y X3 son necesarias. Eliminemos todas las otras que comiencen con X (los rankings):

```{r}
dejandoUno=seq(4,16,2) # 4,6,8, etc.
queColumnas=c(dejandoUno)
provincias=provincias[,-queColumnas]
head(provincias,10)
```

Nuevamente, podemos quedarnos con los casos completos por fila:

```{r}
provincias=provincias[complete.cases(provincias),]
head(provincias,10)
```

Renombremos las dos primeras columnas

```{r}
names(provincias)[c(1,2)]=c('provinciaUbigeo','provinciaNombre')
head(provincias)
```

Con la misma lógica, simplifiquemos los otros nombres de columnas:

```{r}
names(provincias)
```

#abreviemos los nombres de las columnas

```{r}
names(provincias)[c(3:9)]=c('pob2012','ide2012','identificacion2012','medicos2012','escolaridad2012','AguaDesague2012','electrificacion2012')
head(provincias)
```

Podriamos calcular la media de la población:

```{r}
mean(provincias$pob2012)
```

Si aplicas una función numérica, y sale error, es por que quizas los datos tengan problemas:

```{r}
str(provincias)
```

Las funciones numéricas no pueden aplicarse a los textos ('chr'), por ende:

```{r}
provincias[,c(3:9)]=lapply(provincias[,c(3:9)],as.numeric)
str(provincias)
```

Ahora si funciona:

```{r}
mean(provincias$pob2012)
```

Podemos ahora crear la columna regionUbigeo, usando provinciaUbigeo; sólo hay que quedarnos con los dos (2) primeros digitos de esa columna (nótese que están como texto).

```{r}
#primero la duplicamos pero con otro nombre (por defecto nueva columna va al final)
provincias$regionUbigeo=provincias$provinciaUbigeo
```

A la nueva columna le reemplazamos con '0000' todo valor luego de los primeros dos digitos:

```{r}
substr(provincias$regionUbigeo,3,6)='0000'
```

Podemos reubicar la posición de la última columna:

```{r}
# jugando con la posiciones:
provincias=provincias[,c(10,1:9)] 
```

Tenemos entonces:

```{r}
head(provincias,10)
```

Hagamos el 'merge', entre los datos de regiones y provincias:

```{r}
provinciasNew=merge(provincias,regiones,by.x = 'regionUbigeo',
                    by.y='regionUbigeo',
                    all.x = TRUE) # Esto detecta si alguna provincia no encontró Region.
```

Si el merge salió bien, provincias tendrá una nueva columna (al final) con el nombre de la Región:

```{r}
names(provinciasNew)
```

Movamos de posición esa columna nueva:

```{r}
provinciasNew=provinciasNew[,c(1:2,11,3:10)]
```

Asi quedó:

```{r}
head(provinciasNew)
```

Muy bien. Ahora comencemos a pensar en los conglomerados.

La idea es super simple, queremos, en este caso, saber que provincias son lo sufientemente similares entre sí para decir que forman un grupo homogeneo o conglomerado. Lo atractivo, en este caso, es que podemos usar muchas características de las provincias (esta es una técnica multivariada) para agruparlas. Una limitación a tener en cuenta, es que la técnica de conglomerados que veremos está limitada a variables numéricas.

Antes que nada debemos cambiar el nombre de las filas:

```{r}
head(provinciasNew)
```

Arriba, hay numeros en cada fila, ahora deben estar los nombres de las provincias, si es que no comparten el mismo nombre:

```{r}
length(provinciasNew$provinciaNombre)==length(unique(provinciasNew$provinciaNombre))
```

Como no hay nombres repetidos:

```{r}
row.names(provinciasNew)=provinciasNew$provinciaNombre
head(provinciasNew)
```

Ya no están los numeros!

Ahora exploremos las variables:

```{r}
summary(provinciasNew)
```

La función summary de R no ayuda mucho para comparar los valores. Podemos crear nuestra propia función para explorar los datos:

```{r}
df=provinciasNew[c(7:11)]
descriptivos <- data.frame(
  Min = apply(df, 2, min), # minimum
  Med = apply(df, 2, median), # median
  Mean = apply(df, 2, mean), # mean
  SD = apply(df, 2, sd), # Standard deviation
  Max = apply(df, 2, max) # Maximum
  )
descriptivos <- round(descriptivos, 1)
head(descriptivos)
```

Estas son todas las variables numéricas (obviando población y el índice en sí), y nos damos cuenta que:

* Población con acta nacimiento o DNI, es un **porcentaje**.

* Médicos por cada 10,000 habitantes, es una **razón**.

* Tasa de asistencia a Secundaria (Pob. 12 a 16 años), es un **porcentaje**.

* Viviendas con agua y desague, es un **porcentaje**.

* Viviendas electrificadas, es un **porcentaje**.

Por lo general, las características de las unidades de estudio (aqui Provincias) vienen con unidades de medición diferentes. Para que la técnica de comglomerados funcione adecuadamente, debemos llevar las variables a valores independientes de la unidad de medida:

```{r}
df.scaled1 <- scale(df)
head(df.scaled1)
```

Una vez que los datos se han reescalado, debemos generar información sobre las similitudes o 'distancias' entre las filas basándonos en las columnas:

```{r}
d1 <- dist(df.scaled1)
```

En d1 están las distancias, esto permite usar el algoritmo de conglomeración jerárquica.

```{r}
d1.clusters=hclust(d1)
```

Los resultados se exploran visualmente mediante el **dendograma**:

```{r}
plot(d1.clusters,main='Conglomerados',cex=0.4)
```

Como se ve arriba, cada provincia comenzó aislada, y luego se van conglomerando. Uno debe decidir cuantos conglomerados usar, a partir de esta gráfica. Escojamos cuatro conglomerados:

```{r}
grupo <- cutree(d1.clusters, k = 4)
table(grupo)
```

Para ver uno de los conglomerados:

```{r}
row.names(df)[grupo == 4] 
```

O de manera gráfica:

```{r}
plot(d1.clusters, cex = 0.6)
rect.hclust(d1.clusters, k = 4, border = c('red','blue','gray','green'))
```

Nótese que el número asignada a cada conglomerados no representa ningun orden. Aqui observamos el bloque 4:

```{r}
plot(d1.clusters, cex = 0.6)
rect.hclust(d1.clusters, k = 4, border = 'red',which = 4)
```

El dendograma nos muestra el proceso de aglomeración, pero nosotros debemos decidir con cuantos clusters quedarnos.

Lo interesante es que no hay una manera certera de decidir cuántos clusters debemos elegir. De hecho hay diversos indices creados para sugerir cuántos. Estos indices podemos verlos de esta manera:

```{r}
library(NbClust)
nb <- NbClust(df.scaled1, method = "complete") # 'dist' usa este método por defecto
```

Del reporte anterior, se ve la sugerencia depende de la regla de la mayoria. El resultado anterior se puede plotear así:

```{r}
library(factoextra) # toma su tiempo
fviz_nbclust(nb) + theme_minimal()
```

El paquete factoextra tiene su propia función para calcular conglomerados ('eclust').

```{r}
res.hc <- eclust(df.scaled1, FUNcluster ="hclust", k = 4,
                method = "complete", graph = FALSE) 
fviz_dend(res.hc, rect = TRUE, show_labels = FALSE) 
```

Este paquete permite validar qué tan buena ha sido nuestra aglomeración:

```{r}
fviz_silhouette(res.hc)
```

Un valor cercano a **uno** indica que el valor está bien aglomerado, si está cercano a cero indica que esta entre dos conglomerados, y si es negativo, que está en un cluster erróneo (al algoritmo acabó y no pudo ubicarlo en el grupo adecuado).

El objeto *res.hc* guarda esta información de silueta. Para acceder a ella tenemos:

```{r}
siluetas <-res.hc$silinfo$widths
```

Para ver los que están mal agrupados tenemos:

```{r}
siluetas[siluetas$sil_width<0,]
```

Cuando utilizamos la función **scale** para normalizar las variables usamos los valores por defecto de esa función, es decir, la media y la desviación típica. Podríamos usar las medianas y la desviaciónes de la mediana en vez de ellas:

```{r}
medians = apply(df,2,median)
mads = apply(df,2,mad)
df.scaled2 = scale(df,center=medians,scale=mads)
```

Habrán cambios cualitativos si usamos estos nuevos valores?

[Esto ya es adicional]

Veamos:

Hagamos primero el reescalamiento [ya está hecho en la última línea del último código: 

```{r}
head(df.scaled2)
```

Una vez que los datos se han reescalado, debemos generar información sobre las similitudes o 'distancias' entre las filas basándonos en las columnas:

```{r}
d2 <- dist(df.scaled2)
```

En d2 están las distancias, esto permite usar el algoritmo de conglomeración jerárquica.

```{r}
d2.clusters=hclust(d2)
```

Los resultados se exploran visualmente mediante el **dendograma**:

```{r}
plot(d2.clusters,main='Conglomerados',cex=0.4)
```

Como se ve arriba, cada provincia comenzó aislada, y luego se van conglomerando. Uno debe decidir cuantos conglomerados usar, a partir de esta gráfica. Escojamos cuatro conglomerados:

```{r}
grupo2 <- cutree(d2.clusters, k = 4)
table(grupo2)
```

Para ver uno de los conglomerados:

```{r}
row.names(df)[grupo2 == 4] 
```

O de manera gráfica:

```{r}
plot(d2.clusters, cex = 0.6)
rect.hclust(d2.clusters, k = 4, border = c('red','blue','gray','green'))
```

Nótese que el número asignada a cada conglomerados no representa ningun orden. Aqui observamos el bloque 4:

```{r}
plot(d2.clusters, cex = 0.6)
rect.hclust(d2.clusters, k = 4, border = 'red',which = 4)
```

El dendograma nos muestra el proceso de aglomeración, pero nosotros debemos decidir con cuantos clusters quedarnos.

Lo interesante es que no hay una manera certera de decidir cuántos clusters debemos elegir. De hecho hay diversos indices creados para sugerir cuántos. Estos indices podemos verlos de esta manera:

```{r}
library(NbClust)
nb2 <- NbClust(df.scaled2, method = "complete") # 'dist' usa este método por defecto
```

Del reporte anterior, se ve la sugerencia depende de la regla de la mayoria. El resultado anterior se puede plotear así:

```{r}
library(factoextra) # toma su tiempo
fviz_nbclust(nb2) + theme_minimal()
```

El paquete factoextra tiene su propia función para calcular conglomerados ('eclust').

```{r}
res.hc2 <- eclust(df.scaled2, FUNcluster ="hclust", k = 4,
                method = "complete", graph = FALSE) 
fviz_dend(res.hc2, rect = TRUE, show_labels = FALSE) 
```

Este paquete permite validar qué tan buena ha sido nuestra aglomeración:

```{r}
fviz_silhouette(res.hc2)
```

Un valor cercano a uno indica que el valor está bien aglomerado, si está cercano a cero indica que esta entre dos conglomerados, y si es negativo, que está en un cluster erróneo (al algoritmo acabó y no pudo ubicarlo en el grupo adecuado).

El objeto res.hc guarda esta información de silueta. Para acceder a ella tenemos:

```{r}
siluetas2 <-res.hc2$silinfo$widths
```

Para ver los que están mal agrupados tenemos:

```{r}
siluetas2[siluetas2$sil_width<0,]
```


***

#Notas:

El método de clusters requiere de ciertas cosas:

1. UNITLESS: Elimina las unidades

scale()

[Si no le pongo nada, el reescalamiento se hace en Z-Scores]

2. DISTANCIA: Creación de una MATRIZ DE DISTANCIA

dist()

En el código que nos ha dado el profesor, no se puede ver la matriz de distancia, pero la puedes ver aquí:

```{r}
d1
```

